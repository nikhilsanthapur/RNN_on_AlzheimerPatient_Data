{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnMycL6FEJsU",
        "outputId": "c543950c-78d3-4f3a-94dd-4ff92cd14832"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Define the parent folder path\n",
        "parent_folder_path = '/content/drive/MyDrive/6389_Ass3/6389Project3Data&ExampleRNN/6389Project3Data'\n",
        "\n",
        "# Specify file prefix and extension\n",
        "file_prefix = 'raw_fmri_feature_matrix_'\n",
        "file_extension = 'txt'\n",
        "\n",
        "# List to store tensor data for each folder\n",
        "all_tensor_data = []\n",
        "\n",
        "# Iterate over both 'AD' and 'CN'\n",
        "for condition in ['AD', 'CN']:\n",
        "    for patient_num in range(1, 11):\n",
        "        # Construct the folder path based on the condition and patient number\n",
        "        folder_path = os.path.join(parent_folder_path, f'{condition}{patient_num}/fmri_average_signal/')\n",
        "\n",
        "        # Function to read and concatenate text files in a folder\n",
        "        def read_and_concatenate_files(folder_path, file_prefix, file_extension):\n",
        "            data = []\n",
        "\n",
        "            # List all files in the folder that match the pattern\n",
        "            files = [f for f in os.listdir(folder_path) if f.startswith(file_prefix) and f.endswith(file_extension)]\n",
        "\n",
        "            for file in files:\n",
        "                file_path = os.path.join(folder_path, file)\n",
        "\n",
        "                # Read the content of the text file\n",
        "                with open(file_path, 'r') as f:\n",
        "                    content = f.read().split()\n",
        "\n",
        "                # Convert the content to a NumPy array\n",
        "                array_data = np.array(content, dtype=np.float32)\n",
        "\n",
        "                # Append the array to the list\n",
        "                data.append(array_data)\n",
        "\n",
        "            # Stack the list of arrays to create a 2D NumPy array\n",
        "            concatenated_data = np.vstack(data)\n",
        "\n",
        "            # Convert the NumPy array to a PyTorch tensor\n",
        "            tensor_data = torch.tensor(concatenated_data, dtype=torch.float32)\n",
        "\n",
        "            return tensor_data\n",
        "\n",
        "        # Call the function to read and concatenate files\n",
        "        tensor_data = read_and_concatenate_files(folder_path, file_prefix, file_extension)\n",
        "\n",
        "        # Append the tensor data to the list\n",
        "        all_tensor_data.append((tensor_data, 1 if condition == 'AD' else 0))  # Assuming binary classification\n",
        "\n",
        "# Display the shape of the resulting tensors\n",
        "for tensor_data, label in all_tensor_data:\n",
        "    print(f\"Tensor Shape: {tensor_data.shape}, Label: {label}\")\n",
        "    # numpy_array = tensor_data.numpy()\n",
        "    # print(numpy_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TDEx17xHCoP",
        "outputId": "8ced056b-a410-406a-dbf8-702fcde4262f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 1\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n",
            "Tensor Shape: torch.Size([101, 150]), Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stacked RNN Model**"
      ],
      "metadata": {
        "id": "h56YKY8TOU52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "# RNN Model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "ybFkXHBSLesf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "\n",
        "# Assuming all_tensor_data is the list containing tuples of (tensor_data, label)\n",
        "\n",
        "# Concatenate all tensors into a single tensor\n",
        "all_data_tensor = torch.stack([tensor_data for tensor_data, _ in all_tensor_data])\n",
        "all_labels_tensor = torch.tensor([label for _, label in all_tensor_data], dtype=torch.float32)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "dataset = TensorDataset(all_data_tensor, all_labels_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = 150  # Size of each time step\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 1\n",
        "\n",
        "model = RNNModel(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Training Accuracy\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted_train = (outputs.squeeze() > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "training_accuracy = correct_train / total_train\n",
        "print(f\"StackedRNN Training Accuracy: {training_accuracy}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"StackedRNN Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTrBtJeNIs4-",
        "outputId": "6bde2e73-3288-4080-9362-82eba280f7fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StackedRNN Training Accuracy: 1.0\n",
            "StackedRNN Test Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Bidirectional RNN Model**"
      ],
      "metadata": {
        "id": "oPIsBWTSOg1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bidirectional RNN Model\n",
        "class BidirectionalRNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(BidirectionalRNNModel, self).__init__()\n",
        "        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_b8vsbNvNk4f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "\n",
        "# Assuming all_tensor_data is the list containing tuples of (tensor_data, label)\n",
        "\n",
        "# Concatenate all tensors into a single tensor\n",
        "all_data_tensor = torch.stack([tensor_data for tensor_data, _ in all_tensor_data])\n",
        "all_labels_tensor = torch.tensor([label for _, label in all_tensor_data], dtype=torch.float32)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "dataset = TensorDataset(all_data_tensor, all_labels_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = 150  # Size of each time step\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 1\n",
        "\n",
        "model = BidirectionalRNNModel(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Training Accuracy\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted_train = (outputs.squeeze() > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "training_accuracy = correct_train / total_train\n",
        "print(f\"BidirectionalRNN Training Accuracy: {training_accuracy}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"BidirectionalRNN Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofQanSY5LWmm",
        "outputId": "4527bf26-eaf7-4934-c66b-6b503582cb1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BidirectionalRNN Training Accuracy: 1.0\n",
            "BidirectionalRNN Test Accuracy: 0.75\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GRU-based RNN Model**"
      ],
      "metadata": {
        "id": "Ys4iMpTSOnqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU Model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_layers):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=0.2, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply by 2 for bidirectional\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])  # Take the output from the last time step\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "tVGmH0sLN39n"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split, TensorDataset\n",
        "\n",
        "# Assuming all_tensor_data is the list containing tuples of (tensor_data, label)\n",
        "\n",
        "# Concatenate all tensors into a single tensor\n",
        "all_data_tensor = torch.stack([tensor_data for tensor_data, _ in all_tensor_data])\n",
        "all_labels_tensor = torch.tensor([label for _, label in all_tensor_data], dtype=torch.float32)\n",
        "\n",
        "# Split the data into train and test sets\n",
        "dataset = TensorDataset(all_data_tensor, all_labels_tensor)\n",
        "train_size = int(0.8 * len(dataset))\n",
        "test_size = len(dataset) - train_size\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Instantiate the model\n",
        "input_size = 150  # Size of each time step\n",
        "hidden_size = 64\n",
        "output_size = 1\n",
        "num_layers = 1\n",
        "\n",
        "model = GRUModel(input_size, hidden_size, output_size, num_layers)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Training Accuracy\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "    for inputs, labels in train_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted_train = (outputs.squeeze() > 0.5).float()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted_train == labels).sum().item()\n",
        "\n",
        "training_accuracy = correct_train / total_train\n",
        "print(f\"GRU-based RNN Training Accuracy: {training_accuracy}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = model(inputs)\n",
        "        predicted = (outputs.squeeze() > 0.5).float()\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"GRU-based RNN Test Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4UpDtMVN4F1",
        "outputId": "795b9c01-403a-4fe5-82b4-3ffd6260fd4c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRU-based RNN Training Accuracy: 1.0\n",
            "GRU-based RNN Test Accuracy: 0.75\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}